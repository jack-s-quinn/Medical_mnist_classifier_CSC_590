# src/train.py
import os
import random
import argparse
from pathlib import Path
import json
import math
import time
import copy

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, random_split
from torchvision.datasets import ImageFolder
from torchvision import transforms
import timm
from sklearn.metrics import accuracy_score, classification_report
from utils import get_transforms, save_labels

def set_seed(seed=42):
    random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)

def create_dataloaders(data_dir, image_size=224, batch_size=32, val_split=0.2, num_workers=4):
    train_transform = get_transforms(image_size=image_size, train=True)
    val_transform = get_transforms(image_size=image_size, train=False)

    # If the dataset already has train/val folders, prefer that.
    train_folder = Path(data_dir) / "train"
    val_folder = Path(data_dir) / "val"
    if train_folder.exists() and val_folder.exists():
        train_dataset = ImageFolder(str(train_folder), transform=train_transform)
        val_dataset = ImageFolder(str(val_folder), transform=val_transform)
    else:
        # use ImageFolder on the top-level and split
        full = ImageFolder(str(data_dir), transform=train_transform)
        n = len(full)
        n_val = int(n * val_split)
        n_train = n - n_val
        train_dataset, val_dataset = random_split(full, [n_train, n_val],
                                                 generator=torch.Generator().manual_seed(42))
        # val dataset should use val_transform
        val_dataset.dataset.transform = val_transform

    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)

    # labels
    if isinstance(train_dataset, ImageFolder):
        class_to_idx = train_dataset.class_to_idx
    else:
        # random_split wraps the original dataset in subset
        class_to_idx = train_dataset.dataset.class_to_idx

    idx_to_class = {v:k for k,v in class_to_idx.items()}
    return train_loader, val_loader, idx_to_class

def build_model(num_classes, model_name="tf_efficientnet_b0"):
    model = timm.create_model(model_name, pretrained=True, num_classes=num_classes)
    return model

def train_one_epoch(model, loader, criterion, optimizer, device):
    model.train()
    running_loss = 0.0
    preds = []
    targets = []
    for imgs, labels in loader:
        imgs = imgs.to(device)
        labels = labels.to(device)
        optimizer.zero_grad()
        outputs = model(imgs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        running_loss += loss.item() * imgs.size(0)
        preds.extend(outputs.argmax(dim=1).detach().cpu().numpy().tolist())
        targets.extend(labels.detach().cpu().numpy().tolist())
    epoch_loss = running_loss / len(loader.dataset)
    acc = accuracy_score(targets, preds)
    return epoch_loss, acc

def validate(model, loader, criterion, device):
    model.eval()
    running_loss = 0.0
    preds = []
    targets = []
    with torch.no_grad():
        for imgs, labels in loader:
            imgs = imgs.to(device)
            labels = labels.to(device)
            outputs = model(imgs)
            loss = criterion(outputs, labels)
            running_loss += loss.item() * imgs.size(0)
            preds.extend(outputs.argmax(dim=1).cpu().numpy().tolist())
            targets.extend(labels.cpu().numpy().tolist())
    epoch_loss = running_loss / len(loader.dataset)
    acc = accuracy_score(targets, preds)
    return epoch_loss, acc, preds, targets

def main(args):
    set_seed(args.seed)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print("Using device:", device)

    train_loader, val_loader, idx_to_class = create_dataloaders(args.data_dir, image_size=args.img_size,
                                                                 batch_size=args.batch_size, val_split=args.val_split, num_workers=args.num_workers)
    num_classes = len(idx_to_class)
    print("Classes:", num_classes, idx_to_class)

    model = build_model(num_classes=num_classes, model_name=args.model)
    model = model.to(device)

    criterion = nn.CrossEntropyLoss()
    optimizer = optim.AdamW(model.parameters(), lr=args.lr, weight_decay=1e-4)
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs)

    best_val_acc = 0.0
    best_model_wts = copy.deepcopy(model.state_dict())
    history = {"train_loss":[], "train_acc":[], "val_loss":[], "val_acc":[]}

    for epoch in range(1, args.epochs+1):
        t0 = time.time()
        train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, device)
        val_loss, val_acc, val_preds, val_targets = validate(model, val_loader, criterion, device)
        scheduler.step()
        history["train_loss"].append(train_loss)
        history["train_acc"].append(train_acc)
        history["val_loss"].append(val_loss)
        history["val_acc"].append(val_acc)

        print(f"Epoch {epoch}/{args.epochs} - train_loss: {train_loss:.4f} train_acc: {train_acc:.4f} | val_loss: {val_loss:.4f} val_acc: {val_acc:.4f} | time: {time.time()-t0:.1f}s")

        if val_acc > best_val_acc:
            best_val_acc = val_acc
            best_model_wts = copy.deepcopy(model.state_dict())
            print("Saved new best model at epoch", epoch)

    # load best model weights and save
    model.load_state_dict(best_model_wts)
    os.makedirs(args.output_dir, exist_ok=True)
    model_path = os.path.join(args.output_dir, "model.pt")
    torch.save(model.state_dict(), model_path)

    # save labels and config
    labels_path = os.path.join(args.output_dir, "labels.json")
    save_labels([idx_to_class[str(i)] if False else idx_to_class[i] for i in sorted(idx_to_class.keys())], labels_path)
    # But idx_to_class keys might be ints; better:
    labels_sorted = [idx_to_class[i] for i in sorted(idx_to_class.keys())]
    save_labels(labels_sorted, labels_path)
    # simple config
    config = {
        "model_name": args.model,
        "image_size": args.img_size,
        "classes": labels_sorted
    }
    with open(os.path.join(args.output_dir, "config.json"), "w") as f:
        json.dump(config, f, indent=2)

    print("Training complete. Best val acc:", best_val_acc)
    print("Saved model to", model_path)
    print("Saved labels to", labels_path)


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--data_dir", type=str, default="data/Medical-MNIST", help="path to images root")
    parser.add_argument("--output_dir", type=str, default="output", help="where to save model & labels")
    parser.add_argument("--model", type=str, default="tf_efficientnet_b0", help="timm model name")
    parser.add_argument("--img_size", type=int, default=224)
    parser.add_argument("--batch_size", type=int, default=32)
    parser.add_argument("--epochs", type=int, default=10)
    parser.add_argument("--lr", type=float, default=3e-4)
    parser.add_argument("--val_split", type=float, default=0.2)
    parser.add_argument("--seed", type=int, default=42)
    parser.add_argument("--num_workers", type=int, default=4)
    args = parser.parse_args()
    main(args)
